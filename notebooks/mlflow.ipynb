{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import io\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATABASE SETUP (One-time setup)\n",
    "# ==========================================\n",
    "def create_sqlite_mnist(db_path=\"mnist.db\"):\n",
    "    \"\"\"\n",
    "    Downloads MNIST, converts images to bytes, and stores them in SQLite.\n",
    "    \"\"\"\n",
    "    if os.path.exists(db_path):\n",
    "        print(f\"Database {db_path} already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating SQLite database from MNIST dataset...\")\n",
    "    # Download standard MNIST\n",
    "    transform = transforms.ToTensor()\n",
    "    mnist_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table: ID, Label, ImageBlob\n",
    "    cursor.execute('CREATE TABLE IF NOT EXISTS train_data (id INTEGER PRIMARY KEY, label INTEGER, image BLOB)')\n",
    "    \n",
    "    # Batch insert for speed\n",
    "    data_to_insert = []\n",
    "    for idx, (img_tensor, label) in enumerate(mnist_data):\n",
    "        # Convert tensor to numpy, then pickle to bytes\n",
    "        img_np = img_tensor.numpy() \n",
    "        img_bytes = pickle.dumps(img_np)\n",
    "        data_to_insert.append((idx, label, img_bytes))\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processing image {idx}/{len(mnist_data)}...\")\n",
    "\n",
    "    cursor.executemany('INSERT INTO train_data VALUES (?, ?, ?)', data_to_insert)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database creation complete.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CUSTOM DATASET CLASS\n",
    "# ==========================================\n",
    "class SqliteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that reads from SQLite.\n",
    "    \"\"\"\n",
    "    def __init__(self, db_path, table_name=\"train_data\", transform=None):\n",
    "        self.db_path = db_path\n",
    "        self.table_name = table_name\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Connect to get total length\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {self.table_name}\")\n",
    "        self.length = cursor.fetchone()[0]\n",
    "        conn.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open a new connection per thread (SQLite requirement for concurrency)\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Fetch specific row (id is 0-indexed in our insertion logic)\n",
    "        cursor.execute(f\"SELECT label, image FROM {self.table_name} WHERE id=?\", (idx,))\n",
    "        label, img_bytes = cursor.fetchone()\n",
    "        conn.close()\n",
    "        \n",
    "        # Deserialize\n",
    "        img_np = pickle.loads(img_bytes)\n",
    "        img_tensor = torch.from_numpy(img_np)\n",
    "        \n",
    "        return img_tensor, label\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL DEFINITION\n",
    "# ==========================================\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING FUNCTION WITH MLFLOW\n",
    "# ==========================================\n",
    "def train_and_log():\n",
    "    # --- Configuration ---\n",
    "    DB_PATH = \"mnist.db\"\n",
    "    EXPERIMENT_NAME = \"MNIST_SQLite_Experiment\"\n",
    "    REGISTERED_MODEL_NAME = \"MNIST_Classifier_SQLite\"\n",
    "    \n",
    "    # Hyperparameters to log\n",
    "    params = {\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"hidden_size\": 128,\n",
    "        \"dropout\": 0.2,\n",
    "        \"checkpoint_interval\": 1  # Save checkpoint every N epochs\n",
    "    }\n",
    "\n",
    "    # --- Setup ---\n",
    "    # 1. Prepare DB\n",
    "    create_sqlite_mnist(DB_PATH)\n",
    "    \n",
    "    # 2. Setup DataLoaders\n",
    "    full_dataset = SqliteDataset(DB_PATH)\n",
    "    # Split for validaiton\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # 3. Setup MLflow\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    \n",
    "    # Enable System Metrics (CPU/GPU/RAM usage)\n",
    "    mlflow.enable_system_metrics_logging()\n",
    "\n",
    "    # --- Start Run ---\n",
    "    with mlflow.start_run() as run:\n",
    "        print(f\"Starting Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        # Log Hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Initialize Model & Optimizer\n",
    "        model = MnistModel(hidden_size=params[\"hidden_size\"], dropout_rate=params[\"dropout\"])\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "        # Infer Signature (Input/Output Schema)\n",
    "        # Grab a dummy batch to define input shape\n",
    "        dummy_input, _ = next(iter(train_loader))\n",
    "        dummy_output = model(dummy_input)\n",
    "        signature = infer_signature(dummy_input.numpy(), dummy_output.detach().numpy())\n",
    "\n",
    "        # --- 1. Log the Dataset ---\n",
    "        # We grab a single batch to act as a \"schema definition\" and profile\n",
    "        # We point the 'source' to our local sqlite file\n",
    "        sample_data, sample_targets = next(iter(train_loader))\n",
    "        \n",
    "        # Convert to numpy for MLflow interpretation\n",
    "        dataset_source_path = os.path.abspath(DB_PATH)\n",
    "        \n",
    "        # specific MLflow data object\n",
    "        dataset_info = mlflow.data.from_numpy(\n",
    "            features=sample_data.numpy(), \n",
    "            targets=sample_targets.numpy(), \n",
    "            name=\"mnist_sqlite_train\", \n",
    "            source=dataset_source_path  # Links the run to this specific file\n",
    "        )\n",
    "        \n",
    "        # Log it to the run\n",
    "        print(\"Logging dataset info to MLflow...\")\n",
    "        mlflow.log_input(dataset_info, context=\"training\")\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Calculate Epoch Metrics\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_acc = correct / total\n",
    "\n",
    "            # Validation Step\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    output = model(data)\n",
    "                    _, predicted = output.max(1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += predicted.eq(target).sum().item()\n",
    "            val_acc = val_correct / val_total\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "            # Log Metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_accuracy\": val_acc\n",
    "            }, step=epoch)\n",
    "\n",
    "            # Checkpoint Tracking (Log model state every N epochs)\n",
    "            if (epoch + 1) % params[\"checkpoint_interval\"] == 0:\n",
    "                print(f\"Saving checkpoint for epoch {epoch+1}...\")\n",
    "                mlflow.pytorch.log_model(\n",
    "                    pytorch_model=model,\n",
    "                    artifact_path=f\"checkpoints/epoch_{epoch+1}\",\n",
    "                    signature=signature,\n",
    "                    input_example=dummy_input.numpy()  # Saves a sample input file\n",
    "                )\n",
    "\n",
    "        # --- Final Model Logging & Registration ---\n",
    "        print(\"Logging final model...\")\n",
    "        model_info = mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            registered_model_name=REGISTERED_MODEL_NAME  # Automatically creates/updates version in Registry\n",
    "        )\n",
    "        \n",
    "        return model_info, params\n",
    "\n",
    "# ==========================================\n",
    "# 5. LOADING BEST MODEL FOR TESTING\n",
    "# ==========================================\n",
    "def load_and_test_best_model():\n",
    "    EXPERIMENT_NAME = \"MNIST_SQLite_Experiment\"\n",
    "    \n",
    "    print(\"\\n--- Finding Best Model ---\")\n",
    "    \n",
    "    # 1. Search for the best run in this experiment\n",
    "    current_experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if not current_experiment:\n",
    "        print(\"Experiment not found!\")\n",
    "        return\n",
    "\n",
    "    # Search runs, order by 'val_accuracy' descending, take top 1\n",
    "    best_run = mlflow.search_runs(\n",
    "        experiment_ids=[current_experiment.experiment_id],\n",
    "        order_by=[\"metrics.val_accuracy DESC\"],\n",
    "        max_results=1\n",
    "    ).iloc[0]\n",
    "\n",
    "    run_id = best_run.run_id\n",
    "    best_acc = best_run[\"metrics.val_accuracy\"]\n",
    "    print(f\"Best Run ID: {run_id} with Val Accuracy: {best_acc}\")\n",
    "\n",
    "    # 2. Load the model from that run\n",
    "    # Format: runs:/<run_id>/<artifact_path>\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    print(f\"Loading model from: {model_uri}\")\n",
    "    \n",
    "    loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "    \n",
    "    # 3. Test Prediction\n",
    "    print(\"Running inference on random noise...\")\n",
    "    loaded_model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 28, 28) # Single random image\n",
    "    with torch.no_grad():\n",
    "        prediction = loaded_model(dummy_input)\n",
    "        predicted_class = prediction.argmax().item()\n",
    "    \n",
    "    print(f\"Model prediction (class index): {predicted_class}\")\n",
    "    print(\"Success! Workflow complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    train_and_log()\n",
    "    load_and_test_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLI: mlflow server --port 5000\n",
    "HTTP: http://localhost:5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
