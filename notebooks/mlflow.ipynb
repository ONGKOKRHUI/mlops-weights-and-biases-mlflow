{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa90f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SQLite database from MNIST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:14<00:00, 675kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 49.3kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:03<00:00, 519kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.24MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 0/60000...\n",
      "Processing image 1000/60000...\n",
      "Processing image 2000/60000...\n",
      "Processing image 3000/60000...\n",
      "Processing image 4000/60000...\n",
      "Processing image 5000/60000...\n",
      "Processing image 6000/60000...\n",
      "Processing image 7000/60000...\n",
      "Processing image 8000/60000...\n",
      "Processing image 9000/60000...\n",
      "Processing image 10000/60000...\n",
      "Processing image 11000/60000...\n",
      "Processing image 12000/60000...\n",
      "Processing image 13000/60000...\n",
      "Processing image 14000/60000...\n",
      "Processing image 15000/60000...\n",
      "Processing image 16000/60000...\n",
      "Processing image 17000/60000...\n",
      "Processing image 18000/60000...\n",
      "Processing image 19000/60000...\n",
      "Processing image 20000/60000...\n",
      "Processing image 21000/60000...\n",
      "Processing image 22000/60000...\n",
      "Processing image 23000/60000...\n",
      "Processing image 24000/60000...\n",
      "Processing image 25000/60000...\n",
      "Processing image 26000/60000...\n",
      "Processing image 27000/60000...\n",
      "Processing image 28000/60000...\n",
      "Processing image 29000/60000...\n",
      "Processing image 30000/60000...\n",
      "Processing image 31000/60000...\n",
      "Processing image 32000/60000...\n",
      "Processing image 33000/60000...\n",
      "Processing image 34000/60000...\n",
      "Processing image 35000/60000...\n",
      "Processing image 36000/60000...\n",
      "Processing image 37000/60000...\n",
      "Processing image 38000/60000...\n",
      "Processing image 39000/60000...\n",
      "Processing image 40000/60000...\n",
      "Processing image 41000/60000...\n",
      "Processing image 42000/60000...\n",
      "Processing image 43000/60000...\n",
      "Processing image 44000/60000...\n",
      "Processing image 45000/60000...\n",
      "Processing image 46000/60000...\n",
      "Processing image 47000/60000...\n",
      "Processing image 48000/60000...\n",
      "Processing image 49000/60000...\n",
      "Processing image 50000/60000...\n",
      "Processing image 51000/60000...\n",
      "Processing image 52000/60000...\n",
      "Processing image 53000/60000...\n",
      "Processing image 54000/60000...\n",
      "Processing image 55000/60000...\n",
      "Processing image 56000/60000...\n",
      "Processing image 57000/60000...\n",
      "Processing image 58000/60000...\n",
      "Processing image 59000/60000...\n",
      "Database creation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 15:18:41 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/02 15:18:41 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/02 15:18:41 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/02 15:18:41 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/02 15:18:41 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2026/01/02 15:18:41 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2026/01/02 15:18:41 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -> 1bd49d398cd23, add secrets tables\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/02 15:18:42 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/02 15:18:42 INFO mlflow.tracking.fluent: Experiment with name 'MNIST_SQLite_Experiment' does not exist. Creating a new experiment.\n",
      "2026/01/02 15:18:43 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2026/01/02 15:18:43 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run ID: 426cfd29a96a4eee9e6e93aec2af8bde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:148: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for 'c:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\notebooks\\mnist.db'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "c:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\data\\dataset_source_registry.py:148: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging dataset info to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 15:19:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/02 15:19:18 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2026/01/02 15:19:18 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.4049, Val Acc=0.9344\n",
      "Saving checkpoint for epoch 1...\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Invalid model name ('checkpoints/epoch_1') provided. Model name must be a non-empty string and cannot contain the following characters: ('/', ':', '.', '%', '\"', \"'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 298\u001b[39m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSuccess! Workflow complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[43mtrain_and_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     load_and_test_best_model()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mtrain_and_log\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % params[\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_interval\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m    234\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving checkpoint for epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m         \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m            \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcheckpoints/epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Saves a sample input file\u001b[39;49;00m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# --- Final Model Logging & Registration ---\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLogging final model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\pytorch\\__init__.py:288\u001b[39m, in \u001b[36mlog_model\u001b[39m\u001b[34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, extra_files, pip_requirements, extra_pip_requirements, metadata, name, params, tags, model_type, step, model_id, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    285\u001b[39m \u001b[33;03m    PyTorch logged models\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m pickle_module = pickle_module \u001b[38;5;129;01mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpytorch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\models\\model.py:1161\u001b[39m, in \u001b[36mModel.log\u001b[39m\u001b[34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[39m\n\u001b[32m   1159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flavor_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1160\u001b[39m     flavor_name = flavor.\u001b[34m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(flavor, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcustom\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m model = \u001b[43m_create_logged_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO: Update model name\u001b[39;49;00m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m _last_logged_model_id.set(model.model_id)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1174\u001b[39m     MLFLOW_PRINT_MODEL_URLS_ON_CREATION.get()\n\u001b[32m   1175\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_databricks_uri(tracking_uri)\n\u001b[32m   1176\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (workspace_url := get_workspace_url())\n\u001b[32m   1177\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:2405\u001b[39m, in \u001b[36m_create_logged_model\u001b[39m\u001b[34m(name, source_run_id, tags, params, model_type, experiment_id, flavor)\u001b[39m\n\u001b[32m   2401\u001b[39m     experiment_id = _get_experiment_id() \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   2402\u001b[39m         get_run(source_run_id).info.experiment_id \u001b[38;5;28;01mif\u001b[39;00m source_run_id \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2403\u001b[39m     )\n\u001b[32m   2404\u001b[39m resolved_tags = context_registry.resolve_tags(tags)\n\u001b[32m-> \u001b[39m\u001b[32m2405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_logged_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2408\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\tracking\\client.py:5625\u001b[39m, in \u001b[36mMlflowClient._create_logged_model\u001b[39m\u001b[34m(self, experiment_id, name, source_run_id, tags, params, model_type, flavor)\u001b[39m\n\u001b[32m   5615\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_logged_model\u001b[39m(\n\u001b[32m   5616\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5617\u001b[39m     experiment_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5623\u001b[39m     flavor: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5624\u001b[39m ) -> LoggedModel:\n\u001b[32m-> \u001b[39m\u001b[32m5625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_logged_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_run_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflavor\u001b[49m\n\u001b[32m   5627\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\telemetry\\track.py:30\u001b[39m, in \u001b[36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m start_time = time.time()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result  \u001b[38;5;66;03m# noqa: RET504\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:870\u001b[39m, in \u001b[36mTrackingServiceClient.create_logged_model\u001b[39m\u001b[34m(self, experiment_id, name, source_run_id, tags, params, model_type, flavor)\u001b[39m\n\u001b[32m    857\u001b[39m \u001b[38;5;129m@record_usage_event\u001b[39m(CreateLoggedModelEvent)\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_logged_model\u001b[39m(\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m     flavor: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    869\u001b[39m ) -> LoggedModel:\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_logged_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_run_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_run_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mLoggedModelTag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mLoggedModelParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\store\\tracking\\sqlalchemy_store.py:1893\u001b[39m, in \u001b[36mSqlAlchemyStore.create_logged_model\u001b[39m\u001b[34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[39m\n\u001b[32m   1884\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_logged_model\u001b[39m(\n\u001b[32m   1885\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1886\u001b[39m     experiment_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1891\u001b[39m     model_type: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1892\u001b[39m ) -> LoggedModel:\n\u001b[32m-> \u001b[39m\u001b[32m1893\u001b[39m     \u001b[43m_validate_logged_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1894\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ManagedSessionMaker() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m   1895\u001b[39m         experiment = \u001b[38;5;28mself\u001b[39m.get_experiment(experiment_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Documents\\repos\\mlops-weights-&-biases\\venv\\Lib\\site-packages\\mlflow\\utils\\validation.py:689\u001b[39m, in \u001b[36m_validate_logged_model_name\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    687\u001b[39m bad_chars = (\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(c \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m bad_chars):\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    690\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid model name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m) provided. Model name must be a non-empty string \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    691\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand cannot contain the following characters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbad_chars\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    692\u001b[39m         INVALID_PARAMETER_VALUE,\n\u001b[32m    693\u001b[39m     )\n",
      "\u001b[31mMlflowException\u001b[39m: Invalid model name ('checkpoints/epoch_1') provided. Model name must be a non-empty string and cannot contain the following characters: ('/', ':', '.', '%', '\"', \"'\")"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import io\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATABASE SETUP (One-time setup)\n",
    "# ==========================================\n",
    "def create_sqlite_mnist(db_path=\"mnist.db\"):\n",
    "    \"\"\"\n",
    "    Downloads MNIST, converts images to bytes, and stores them in SQLite.\n",
    "    \"\"\"\n",
    "    if os.path.exists(db_path):\n",
    "        print(f\"Database {db_path} already exists. Skipping creation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating SQLite database from MNIST dataset...\")\n",
    "    # Download standard MNIST\n",
    "    transform = transforms.ToTensor()\n",
    "    mnist_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table: ID, Label, ImageBlob\n",
    "    cursor.execute('CREATE TABLE IF NOT EXISTS train_data (id INTEGER PRIMARY KEY, label INTEGER, image BLOB)')\n",
    "    \n",
    "    # Batch insert for speed\n",
    "    data_to_insert = []\n",
    "    for idx, (img_tensor, label) in enumerate(mnist_data):\n",
    "        # Convert tensor to numpy, then pickle to bytes\n",
    "        img_np = img_tensor.numpy() \n",
    "        img_bytes = pickle.dumps(img_np)\n",
    "        data_to_insert.append((idx, label, img_bytes))\n",
    "        \n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processing image {idx}/{len(mnist_data)}...\")\n",
    "\n",
    "    cursor.executemany('INSERT INTO train_data VALUES (?, ?, ?)', data_to_insert)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database creation complete.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CUSTOM DATASET CLASS\n",
    "# ==========================================\n",
    "class SqliteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset that reads from SQLite.\n",
    "    \"\"\"\n",
    "    def __init__(self, db_path, table_name=\"train_data\", transform=None):\n",
    "        self.db_path = db_path\n",
    "        self.table_name = table_name\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Connect to get total length\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {self.table_name}\")\n",
    "        self.length = cursor.fetchone()[0]\n",
    "        conn.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open a new connection per thread (SQLite requirement for concurrency)\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Fetch specific row (id is 0-indexed in our insertion logic)\n",
    "        cursor.execute(f\"SELECT label, image FROM {self.table_name} WHERE id=?\", (idx,))\n",
    "        label, img_bytes = cursor.fetchone()\n",
    "        conn.close()\n",
    "        \n",
    "        # Deserialize\n",
    "        img_np = pickle.loads(img_bytes)\n",
    "        img_tensor = torch.from_numpy(img_np)\n",
    "        \n",
    "        return img_tensor, label\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL DEFINITION\n",
    "# ==========================================\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self, hidden_size=128, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING FUNCTION WITH MLFLOW\n",
    "# ==========================================\n",
    "def train_and_log():\n",
    "    # --- Configuration ---\n",
    "    DB_PATH = \"mnist.db\"\n",
    "    EXPERIMENT_NAME = \"MNIST_SQLite_Experiment\"\n",
    "    REGISTERED_MODEL_NAME = \"MNIST_Classifier_SQLite\"\n",
    "    \n",
    "    # Hyperparameters to log\n",
    "    params = {\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"hidden_size\": 128,\n",
    "        \"dropout\": 0.2,\n",
    "        \"checkpoint_interval\": 1  # Save checkpoint every N epochs\n",
    "    }\n",
    "\n",
    "    # --- Setup ---\n",
    "    # 1. Prepare DB\n",
    "    create_sqlite_mnist(DB_PATH)\n",
    "    \n",
    "    # 2. Setup DataLoaders\n",
    "    full_dataset = SqliteDataset(DB_PATH)\n",
    "    # Split for validaiton\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # 3. Setup MLflow\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    \n",
    "    # Enable System Metrics (CPU/GPU/RAM usage)\n",
    "    mlflow.enable_system_metrics_logging()\n",
    "\n",
    "    # --- Start Run ---\n",
    "    with mlflow.start_run() as run:\n",
    "        print(f\"Starting Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        # Log Hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Initialize Model & Optimizer\n",
    "        model = MnistModel(hidden_size=params[\"hidden_size\"], dropout_rate=params[\"dropout\"])\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "        # Infer Signature (Input/Output Schema)\n",
    "        # Grab a dummy batch to define input shape\n",
    "        dummy_input, _ = next(iter(train_loader))\n",
    "        dummy_output = model(dummy_input)\n",
    "        signature = infer_signature(dummy_input.numpy(), dummy_output.detach().numpy())\n",
    "\n",
    "        # --- 1. Log the Dataset ---\n",
    "        # We grab a single batch to act as a \"schema definition\" and profile\n",
    "        # We point the 'source' to our local sqlite file\n",
    "        sample_data, sample_targets = next(iter(train_loader))\n",
    "        \n",
    "        # Convert to numpy for MLflow interpretation\n",
    "        dataset_source_path = os.path.abspath(DB_PATH)\n",
    "        \n",
    "        # specific MLflow data object\n",
    "        dataset_info = mlflow.data.from_numpy(\n",
    "            features=sample_data.numpy(), \n",
    "            targets=sample_targets.numpy(), \n",
    "            name=\"mnist_sqlite_train\", \n",
    "            source=dataset_source_path  # Links the run to this specific file\n",
    "        )\n",
    "        \n",
    "        # Log it to the run\n",
    "        print(\"Logging dataset info to MLflow...\")\n",
    "        mlflow.log_input(dataset_info, context=\"training\")\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        for epoch in range(params[\"epochs\"]):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Calculate Epoch Metrics\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_acc = correct / total\n",
    "\n",
    "            # Validation Step\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    output = model(data)\n",
    "                    _, predicted = output.max(1)\n",
    "                    val_total += target.size(0)\n",
    "                    val_correct += predicted.eq(target).sum().item()\n",
    "            val_acc = val_correct / val_total\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "            # Log Metrics\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_accuracy\": val_acc\n",
    "            }, step=epoch)\n",
    "\n",
    "            # Checkpoint Tracking (Log model state every N epochs)\n",
    "            if (epoch + 1) % params[\"checkpoint_interval\"] == 0:\n",
    "                print(f\"Saving checkpoint for epoch {epoch+1}...\")\n",
    "                mlflow.pytorch.log_model(\n",
    "                    pytorch_model=model,\n",
    "                    artifact_path=f\"checkpoints/epoch_{epoch+1}\",\n",
    "                    signature=signature,\n",
    "                    input_example=dummy_input.numpy()  # Saves a sample input file\n",
    "                )\n",
    "\n",
    "        # --- Final Model Logging & Registration ---\n",
    "        print(\"Logging final model...\")\n",
    "        model_info = mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            registered_model_name=REGISTERED_MODEL_NAME  # Automatically creates/updates version in Registry\n",
    "        )\n",
    "        \n",
    "        return model_info, params\n",
    "\n",
    "# ==========================================\n",
    "# 5. LOADING BEST MODEL FOR TESTING\n",
    "# ==========================================\n",
    "def load_and_test_best_model():\n",
    "    EXPERIMENT_NAME = \"MNIST_SQLite_Experiment\"\n",
    "    \n",
    "    print(\"\\n--- Finding Best Model ---\")\n",
    "    \n",
    "    # 1. Search for the best run in this experiment\n",
    "    current_experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if not current_experiment:\n",
    "        print(\"Experiment not found!\")\n",
    "        return\n",
    "\n",
    "    # Search runs, order by 'val_accuracy' descending, take top 1\n",
    "    best_run = mlflow.search_runs(\n",
    "        experiment_ids=[current_experiment.experiment_id],\n",
    "        order_by=[\"metrics.val_accuracy DESC\"],\n",
    "        max_results=1\n",
    "    ).iloc[0]\n",
    "\n",
    "    run_id = best_run.run_id\n",
    "    best_acc = best_run[\"metrics.val_accuracy\"]\n",
    "    print(f\"Best Run ID: {run_id} with Val Accuracy: {best_acc}\")\n",
    "\n",
    "    # 2. Load the model from that run\n",
    "    # Format: runs:/<run_id>/<artifact_path>\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    print(f\"Loading model from: {model_uri}\")\n",
    "    \n",
    "    loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "    \n",
    "    # 3. Test Prediction\n",
    "    print(\"Running inference on random noise...\")\n",
    "    loaded_model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 28, 28) # Single random image\n",
    "    with torch.no_grad():\n",
    "        prediction = loaded_model(dummy_input)\n",
    "        predicted_class = prediction.argmax().item()\n",
    "    \n",
    "    print(f\"Model prediction (class index): {predicted_class}\")\n",
    "    print(\"Success! Workflow complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    train_and_log()\n",
    "    load_and_test_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82d5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLI: mlflow server --port 5000\n",
    "HTTP: http://localhost:5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
